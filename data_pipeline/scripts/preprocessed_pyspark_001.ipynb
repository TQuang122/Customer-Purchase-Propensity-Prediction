{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6550aa64",
   "metadata": {},
   "source": [
    "# Purchase Propensity Data Preprocessing (PySpark)\n",
    "\n",
    "**Objective**: Transform raw events into a training table where 1 row = 1 November cart event.\n",
    "\n",
    "**Strategy**:\n",
    "- Training samples: Cart events from November only\n",
    "- Target: `is_purchased` = 1 if user purchases same product after cart event\n",
    "\n",
    "**Technology**: PySpark for distributed processing of large datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b21de430",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1124b955",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Using incubator modules: jdk.incubator.vector\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "26/01/18 20:36:25 WARN Utils: Your hostname, MacBook-Pro-cua-Le-3.local, resolves to a loopback address: 127.0.0.1; using 192.168.1.5 instead (on interface en0)\n",
      "26/01/18 20:36:25 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "26/01/18 20:36:26 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark version: 4.1.1\n",
      "Spark UI: http://192.168.1.5:4040\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession, Window\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import StructType, StructField, StringType, LongType, DoubleType, TimestampType\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Initialize Spark Session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Purchase Propensity Preprocessing\") \\\n",
    "    .config(\"spark.driver.memory\", \"8g\") \\\n",
    "    .config(\"spark.sql.shuffle.partitions\", \"200\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "print(f\"Spark version: {spark.version}\")\n",
    "print(f\"Spark UI: {spark.sparkContext.uiWebUrl}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7761022e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading November data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 0:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total: 67,501,979 rows\n",
      "root\n",
      " |-- event_time: timestamp (nullable = true)\n",
      " |-- event_type: string (nullable = true)\n",
      " |-- product_id: long (nullable = true)\n",
      " |-- category_id: long (nullable = true)\n",
      " |-- category_code: string (nullable = true)\n",
      " |-- brand: string (nullable = true)\n",
      " |-- price: double (nullable = true)\n",
      " |-- user_id: long (nullable = true)\n",
      " |-- user_session: string (nullable = true)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Define schema for data loading\n",
    "schema = StructType([\n",
    "    StructField(\"event_time\", TimestampType(), True),\n",
    "    StructField(\"event_type\", StringType(), True),\n",
    "    StructField(\"product_id\", LongType(), True),\n",
    "    StructField(\"category_id\", LongType(), True),\n",
    "    StructField(\"category_code\", StringType(), True),\n",
    "    StructField(\"brand\", StringType(), True),\n",
    "    StructField(\"price\", DoubleType(), True),\n",
    "    StructField(\"user_id\", LongType(), True),\n",
    "    StructField(\"user_session\", StringType(), True)\n",
    "])\n",
    "\n",
    "# Load November data\n",
    "print(\"Loading November data\")\n",
    "df = spark.read.csv(\n",
    "    '../data/raw/2019-Nov.csv.gz',\n",
    "    schema=schema,\n",
    "    header=True\n",
    ")\n",
    "\n",
    "nov_count = df.count()\n",
    "print(f\"Total: {nov_count:,} rows\")\n",
    "\n",
    "# Display schema\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5539626a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+----------+----------+-------------------+-------------------------+------+------+---------+------------------------------------+\n",
      "|event_time         |event_type|product_id|category_id        |category_code            |brand |price |user_id  |user_session                        |\n",
      "+-------------------+----------+----------+-------------------+-------------------------+------+------+---------+------------------------------------+\n",
      "|2019-11-01 07:00:00|view      |1003461   |2053013555631882655|electronics.smartphone   |xiaomi|489.07|520088904|4d3b30da-a5e4-49df-b1a8-ba5943f1dd33|\n",
      "|2019-11-01 07:00:00|view      |5000088   |2053013566100866035|appliances.sewing_machine|janome|293.65|530496790|8e5f4f83-366c-4f70-860e-ca7417414283|\n",
      "|2019-11-01 07:00:01|view      |17302664  |2053013553853497655|NULL                     |creed |28.31 |561587266|755422e7-9040-477b-9bd2-6a6e8fd97387|\n",
      "|2019-11-01 07:00:01|view      |3601530   |2053013563810775923|appliances.kitchen.washer|lg    |712.87|518085591|3bfb58cd-7892-48cc-8020-2f17e6de6e7f|\n",
      "|2019-11-01 07:00:01|view      |1004775   |2053013555631882655|electronics.smartphone   |xiaomi|183.27|558856683|313628f1-68b8-460d-84f6-cec7a8796ef2|\n",
      "+-------------------+----------+----------+-------------------+-------------------------+------+------+---------+------------------------------------+\n",
      "only showing top 5 rows\n"
     ]
    }
   ],
   "source": [
    "# Preview data\n",
    "df.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceb52033",
   "metadata": {},
   "source": [
    "## 3. Data Profiling & Cleaning Steps\n",
    "\n",
    "This block performs basic **EDA checks** and **data cleaning** to prepare the event log for downstream labeling/analysis.\n",
    "\n",
    "**1) Dataset inspection**\n",
    "- Prints total row count.\n",
    "- Shows the distribution of `event_type` (counts per event type).\n",
    "- Reports missing values per column by counting `NULL`s.\n",
    "- Checks duplicates by comparing `df.count()` vs `df.distinct().count()`.\n",
    "\n",
    "**2) Deduplication**\n",
    "- Removes duplicate events using a key set:\n",
    "  `user_id`, `user_session`, `event_time`, `event_type`, `product_id`\n",
    "- Keeps only one record per unique key combination.\n",
    "\n",
    "**3) Session validity filtering**\n",
    "- Drops rows where `user_session` is missing or equals the string `\"NULL\"`.\n",
    "\n",
    "**4) Normalization for categorical fields**\n",
    "- Standardizes `brand` and `category_code` by:\n",
    "  - converting to lowercase\n",
    "  - filling missing values with `\"unknown\"`\n",
    "\n",
    "**5) Basic cardinality + top categories**\n",
    "- Computes the number of distinct `brand` and `category_code`.\n",
    "- Displays the top 10 most frequent brands and category codes.\n",
    "\n",
    "**6) Ordering and caching**\n",
    "- Sorts the data by `user_id`, `product_id`, `event_time` (important for time-based logic later).\n",
    "- Triggers caching via `df.count()` to speed up subsequent transformations/actions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "039dcaee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial dataset:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total rows: 67,501,979\n",
      "\n",
      "Event type distribution:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------+\n",
      "|event_type|   count|\n",
      "+----------+--------+\n",
      "|      view|63556110|\n",
      "|      cart| 3028930|\n",
      "|  purchase|  916939|\n",
      "+----------+--------+\n",
      "\n",
      "\n",
      "Missing values:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 10:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+----------+-----------+-------------+-------+-----+-------+------------+\n",
      "|event_time|event_type|product_id|category_id|category_code|  brand|price|user_id|user_session|\n",
      "+----------+----------+----------+-----------+-------------+-------+-----+-------+------------+\n",
      "|         0|         0|         0|          0|     21898171|9218235|    0|      0|          10|\n",
      "+----------+----------+----------+-----------+-------------+-------+-----+-------+------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "print(\"Initial dataset:\")\n",
    "print(f\"Total rows: {df.count():,}\")\n",
    "print(f\"\\nEvent type distribution:\")\n",
    "df.groupBy('event_type').count().orderBy('count', ascending=False).show()\n",
    "\n",
    "print(f\"\\nMissing values:\")\n",
    "df.select([F.count(F.when(F.col(c).isNull(), c)).alias(c) for c in df.columns]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "405be19d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 18:=====================================================> (98 + 2) / 100]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total rows    : 67501979\n",
      "Distinct rows : 67401460\n",
      "Duplicate rows: 100519\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "total_rows = df.count()\n",
    "\n",
    "distinct_rows = df.distinct().count()\n",
    "\n",
    "print(f\"Total rows    : {total_rows}\")\n",
    "print(f\"Distinct rows : {distinct_rows}\")\n",
    "print(f\"Duplicate rows: {total_rows - distinct_rows}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3b422030",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove duplicate rows (Deduplication)\n",
    "dedup_keys = [\n",
    "    \"user_id\",\n",
    "    \"user_session\",\n",
    "    \"event_time\",\n",
    "    \"event_type\",\n",
    "    \"product_id\"\n",
    "]\n",
    "\n",
    "df = df.dropDuplicates(dedup_keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "18259fa9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After deduplication:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 24:===================================================>    (62 + 5) / 67]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total rows: 67401449\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "print(\"After deduplication:\")\n",
    "print(\"Total rows:\", df.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "06a94dce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropping rows with missing user_session\n",
      "Normalizing for brand and category_code\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique brands: 4202\n",
      "Unique category_codes: 130\n",
      "Top 10 brands:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+\n",
      "|  brand|  count|\n",
      "+-------+-------+\n",
      "|unknown|9209175|\n",
      "|samsung|7864082|\n",
      "|  apple|6246469|\n",
      "| xiaomi|4630595|\n",
      "| huawei|1407728|\n",
      "|lucente|1183398|\n",
      "|     lg|1094788|\n",
      "|  bosch| 973939|\n",
      "|   oppo| 809948|\n",
      "|   sony| 797670|\n",
      "+-------+-------+\n",
      "only showing top 10 rows\n",
      "Top 10 category_codes:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 56:==================================================>    (91 + 9) / 100]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------+\n",
      "|       category_code|   count|\n",
      "+--------------------+--------+\n",
      "|             unknown|21871420|\n",
      "|electronics.smart...|16333971|\n",
      "|electronics.video.tv| 2203947|\n",
      "|  computers.notebook| 2178918|\n",
      "|  electronics.clocks| 2085555|\n",
      "|       apparel.shoes| 1885774|\n",
      "|electronics.audio...| 1814146|\n",
      "|appliances.enviro...| 1525426|\n",
      "|appliances.kitche...| 1425046|\n",
      "|appliances.kitche...| 1400187|\n",
      "+--------------------+--------+\n",
      "only showing top 10 rows\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "print(\"Dropping rows with missing user_session\")\n",
    "print(\"Normalizing for brand and category_code\")\n",
    "\n",
    "# Drop rows where user_session is null or string \"NULL\"\n",
    "df = df.filter(\n",
    "    (F.col(\"user_session\").isNotNull()) & (F.col(\"user_session\") != \"NULL\")\n",
    ")\n",
    "\n",
    "df = (\n",
    "    df\n",
    "    .withColumn(\n",
    "        \"brand\",\n",
    "        F.coalesce(F.lower(F.col(\"brand\")), F.lit(\"unknown\"))\n",
    "    )\n",
    "    .withColumn(\n",
    "        \"category_code\",\n",
    "        F.coalesce(F.lower(F.col(\"category_code\")), F.lit(\"unknown\"))\n",
    "    )\n",
    ")\n",
    "\n",
    "# cache if reused many times\n",
    "# df.cache()\n",
    "\n",
    "brand_count = df.select(\"brand\").distinct().count()\n",
    "category_count = df.select(\"category_code\").distinct().count()\n",
    "\n",
    "print(f\"Unique brands: {brand_count}\")\n",
    "print(f\"Unique category_codes: {category_count}\")\n",
    "\n",
    "print(\"Top 10 brands:\")\n",
    "df.groupBy(\"brand\").count().orderBy(F.desc(\"count\")).show(10)\n",
    "\n",
    "print(\"Top 10 category_codes:\")\n",
    "df.groupBy(\"category_code\").count().orderBy(F.desc(\"count\")).show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fa9ff112",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sorting data\n",
      "Data sorted by user_id, product_id, event_time\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 62:=============================================>         (56 + 11) / 67]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data cached in memory\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Sort by user_id, product_id, event_time (critical for labeling)\n",
    "print(\"Sorting data\")\n",
    "df = df.orderBy('user_id', 'product_id', 'event_time')\n",
    "print(\"Data sorted by user_id, product_id, event_time\")\n",
    "\n",
    "df.count()  # Trigger caching\n",
    "print(\"Data cached in memory\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71c89675",
   "metadata": {},
   "source": [
    "## 4. Label Engineering (`is_purchased`)\n",
    "\n",
    "**Cart-to-Purchase Attribution Logic**\n",
    "\n",
    "This pipeline attributes each **purchase** event to the **most recent preceding cart** event within the same **user**, **product**, and **session**.\n",
    "\n",
    "**Method overview:**\n",
    "- Split the data into `cart` and `purchase` events.\n",
    "- Join carts to purchases where they share the same user, product, and session, and where the cart happened before the purchase.\n",
    "- For each purchase, select the **nearest cart in time** before it using a window function.\n",
    "- Label those carts as `is_purchased = 1`; all other carts receive `0`.\n",
    "- Compute summary statistics: total carts, credited carts, and cart-to-purchase conversion rate.\n",
    "\n",
    "This ensures a **last-cart-before-purchase** attribution model at session level."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f48fbf79",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 79:>                                                       (0 + 14) / 15]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total cart events: 2,933,439\n",
      "Credited cart events: 763,334\n",
      "Conversion rate: 26.02%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# 1. Tách carts và purchases\n",
    "carts = df.filter(F.col(\"event_type\") == \"cart\")\n",
    "purchases = df.filter(F.col(\"event_type\") == \"purchase\")\n",
    "\n",
    "# 2. Join cart với purchase cùng user, product, session\n",
    "#    và cart_time < purchase_time\n",
    "cart_purchase = (\n",
    "    carts.alias(\"c\")\n",
    "    .join(\n",
    "        purchases.alias(\"p\"),\n",
    "        on=[\n",
    "            F.col(\"c.user_id\") == F.col(\"p.user_id\"),\n",
    "            F.col(\"c.product_id\") == F.col(\"p.product_id\"),\n",
    "            F.col(\"c.user_session\") == F.col(\"p.user_session\"),\n",
    "            F.col(\"c.event_time\") < F.col(\"p.event_time\"),\n",
    "        ],\n",
    "        how=\"inner\"\n",
    "    )\n",
    ")\n",
    "\n",
    "# 3. Với mỗi purchase, chọn cart gần nhất phía trước\n",
    "w_last_cart = (\n",
    "    Window\n",
    "    .partitionBy(\n",
    "        F.col(\"p.user_id\"),\n",
    "        F.col(\"p.product_id\"),\n",
    "        F.col(\"p.user_session\"),\n",
    "        F.col(\"p.event_time\")   # anchor = purchase\n",
    "    )\n",
    "    .orderBy(F.col(\"c.event_time\").desc())\n",
    ")\n",
    "\n",
    "cart_purchase = cart_purchase.withColumn(\n",
    "    \"rank_to_purchase\",\n",
    "    F.row_number().over(w_last_cart)\n",
    ")\n",
    "\n",
    "# 4. Cart được credit (rank = 1)\n",
    "credited_carts = (\n",
    "    cart_purchase\n",
    "    .filter(F.col(\"rank_to_purchase\") == 1)\n",
    "    .select(\n",
    "        F.col(\"c.user_id\").alias(\"user_id\"),\n",
    "        F.col(\"c.product_id\").alias(\"product_id\"),\n",
    "        F.col(\"c.user_session\").alias(\"user_session\"),\n",
    "        F.col(\"c.event_time\").alias(\"event_time\")\n",
    "    )\n",
    "    .withColumn(\"is_purchased\", F.lit(1))\n",
    ")\n",
    "\n",
    "# 5. Gán label về dataframe gốc\n",
    "df = (\n",
    "    df.join(\n",
    "        credited_carts,\n",
    "        on=[\"user_id\", \"product_id\", \"user_session\", \"event_time\"],\n",
    "        how=\"left\"\n",
    "    )\n",
    "    .withColumn(\"is_purchased\", F.coalesce(F.col(\"is_purchased\"), F.lit(0)))\n",
    ")\n",
    "\n",
    "# 6. Stats\n",
    "cart_stats = (\n",
    "    df.filter(F.col(\"event_type\") == \"cart\")\n",
    "      .agg(\n",
    "          F.count(\"*\").alias(\"total_carts\"),\n",
    "          F.sum(\"is_purchased\").alias(\"credited_carts\"),\n",
    "          F.avg(\"is_purchased\").alias(\"conversion_rate\")\n",
    "      )\n",
    "      .collect()[0]\n",
    ")\n",
    "\n",
    "print(f\"Total cart events: {cart_stats['total_carts']:,}\")\n",
    "print(f\"Credited cart events: {cart_stats['credited_carts']:,}\")\n",
    "print(f\"Conversion rate: {cart_stats['conversion_rate']:.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "982070c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop Helper Columns\n",
    "df = df.drop(\n",
    "    \"purchase_time\",\n",
    "    \"has_future_purchase\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "001bb9dd",
   "metadata": {},
   "source": [
    "## 5. Split Category Code\n",
    "\n",
    "- Split `category_code` into `category_code_level1` and `category_code_level2`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "964a1520",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.withColumn(\n",
    "    \"category_code_level1\",\n",
    "    F.when(F.col(\"category_code\").isNull(), F.lit(\"unknown\"))\n",
    "     .otherwise(F.split(F.col(\"category_code\"), r\"\\.\")[0])\n",
    ")\n",
    "\n",
    "df = df.withColumn(\n",
    "    \"category_code_level2\",\n",
    "    F.when(F.col(\"category_code\").isNull(), F.lit(\"unknown\"))\n",
    "     .otherwise(\n",
    "         F.when(F.size(F.split(F.col(\"category_code\"), r\"\\.\")) > 1,\n",
    "                F.split(F.col(\"category_code\"), r\"\\.\")[1]\n",
    "         ).otherwise(F.lit(\"unknown\"))\n",
    "     )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "060d6b31",
   "metadata": {},
   "source": [
    "## 6. Create Event Weekday\n",
    "\n",
    "- Create `event_weekday` column from `event_time`\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a56c89d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Monday = 0, Sunday = 6\n",
    "df = df.withColumn(\n",
    "    \"event_weekday\",\n",
    "    F.dayofweek(\"event_time\") - 2\n",
    ")\n",
    "\n",
    "# Fix Sunday (Spark: Sunday = 1 → -1)\n",
    "df = df.withColumn(\n",
    "    \"event_weekday\",\n",
    "    F.when(F.col(\"event_weekday\") == -1, 6)\n",
    "     .otherwise(F.col(\"event_weekday\"))\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79c39c33",
   "metadata": {},
   "source": [
    "## 7. Create Activity Count (session-level engagement)\n",
    "\n",
    "- Create `activity_count` column from `event_time`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "62d5185b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Window theo session, nhìn về quá khứ\n",
    "w_session_past = (\n",
    "    Window\n",
    "    .partitionBy(\"user_session\")\n",
    "    .orderBy(\"event_time\")\n",
    "    .rowsBetween(Window.unboundedPreceding, 0)\n",
    ")\n",
    "\n",
    "# Đếm số event đến thời điểm hiện tại\n",
    "df = df.withColumn(\n",
    "    \"activity_count\",\n",
    "    F.count(\"*\").over(w_session_past)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0476f56c",
   "metadata": {},
   "source": [
    "## 8. Final Sample Definition\n",
    "\n",
    "- Filter cart events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9d7352b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtering to cart events\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training samples (Nov cart events, price > 0): 2,933,439\n",
      "Positive samples (is_purchased=1): 763,334\n",
      "Conversion rate: 26.02%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 158:=================================================>  (190 + 10) / 200]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training samples cached\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "print(\"Filtering to cart events\")\n",
    "\n",
    "training_samples = df.filter((F.col(\"event_type\") == \"cart\"))\n",
    "\n",
    "training_stats = training_samples.agg(\n",
    "    F.count(\"*\").alias(\"total_samples\"),\n",
    "    F.sum(\"is_purchased\").alias(\"positive_samples\"),\n",
    "    F.avg(\"is_purchased\").alias(\"conversion_rate\")\n",
    ").collect()[0]\n",
    "\n",
    "print(f\"Training samples (Nov cart events, price > 0): {training_stats['total_samples']:,}\")\n",
    "print(f\"Positive samples (is_purchased=1): {training_stats['positive_samples']:,}\")\n",
    "print(f\"Conversion rate: {training_stats['conversion_rate']:.2%}\")\n",
    "\n",
    "training_samples = training_samples.cache()\n",
    "training_samples.count()\n",
    "print(\"\\nTraining samples cached\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b75c0870",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final dataset summary:\n",
      "Total training samples: 2,933,439\n",
      "Positive class (is_purchased=1): 763,334 (26.02%)\n",
      "Negative class (is_purchased=0): 2,170,105 (73.98%)\n",
      "\n",
      "Features available: ['user_id', 'product_id', 'user_session', 'event_time', 'event_type', 'category_id', 'category_code', 'brand', 'price', 'is_purchased', 'category_code_level1', 'category_code_level2', 'event_weekday', 'activity_count']\n"
     ]
    }
   ],
   "source": [
    "#Final dataset summary\n",
    "print(\"Final dataset summary:\")\n",
    "\n",
    "final_stats = training_samples.agg(\n",
    "    F.count('*').alias('total'),\n",
    "    F.sum('is_purchased').alias('positive'),\n",
    "    F.sum(F.when(F.col('is_purchased') == 0, 1).otherwise(0)).alias('negative'),\n",
    "    F.avg('is_purchased').alias('positive_rate')\n",
    ").collect()[0]\n",
    "\n",
    "print(f\"Total training samples: {final_stats['total']:,}\")\n",
    "print(f\"Positive class (is_purchased=1): {final_stats['positive']:,} ({final_stats['positive_rate']:.2%})\")\n",
    "print(f\"Negative class (is_purchased=0): {final_stats['negative']:,} ({1-final_stats['positive_rate']:.2%})\")\n",
    "print(f\"\\nFeatures available: {training_samples.columns}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "762a4095",
   "metadata": {},
   "source": [
    "## 9. Save Processed Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "aae53a4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 2,933,439 rows to ../data/processed/df_processed_pyspark_v1.csv\n"
     ]
    }
   ],
   "source": [
    "# Select relevant columns for output\n",
    "# event_time is required for Feast's event_timestamp\n",
    "output_columns = [\n",
    "    \"event_time\",\n",
    "    \"user_id\",\n",
    "    \"product_id\",\n",
    "    \"category_code_level1\",\n",
    "    \"category_code_level2\",\n",
    "    \"brand\",\n",
    "    \"event_weekday\",\n",
    "    \"price\",\n",
    "    \"activity_count\",\n",
    "    \"is_purchased\"\n",
    "]\n",
    "\n",
    "# Sample and convert to Pandas for single CSV file output\n",
    "df_output = (\n",
    "    training_samples\n",
    "    .select(*output_columns)\n",
    "    .toPandas()\n",
    ")\n",
    "\n",
    "# Save as single CSV file (not folder)\n",
    "output_path = \"../data/processed/df_processed_pyspark_v1.csv\"\n",
    "df_output.to_csv(output_path, index=False)\n",
    "print(f\"Saved {len(df_output):,} rows to {output_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c6cd36a",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "✅ **Completed Steps**:\n",
    "1. Initialized PySpark session with optimized configuration\n",
    "2. Loaded Nov + Dec data with explicit schema definition\n",
    "3. Preprocessed data (null removal, brand normalization, sorting)\n",
    "4. Engineered `is_purchased` label using window functions\n",
    "5. Split Category Code into Level 1 and Level 2\n",
    "6. Computed anti-leakage session features using window functions\n",
    "7. Verified labeling correctness and data quality\n",
    "8. Saved processed training data (CSV and Parquet formats)\n",
    "\n",
    "**PySpark Advantages**:\n",
    "- Distributed processing for large datasets\n",
    "- Lazy evaluation for query optimization\n",
    "- Automatic memory management\n",
    "- Native support for window functions\n",
    "- Parquet format for efficient storage and retrieval\n",
    "\n",
    "**Next Steps**:\n",
    "- Perform EDA on processed data\n",
    "- Engineer additional features (user history, product popularity, etc.)\n",
    "- Train purchase propensity models using Spark MLlib\n",
    "- Evaluate model performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "94c23c20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Clean up and stop Spark session\n",
    "# # Uncomment when done\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64c49cae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffabbcd2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91050d85",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ea80c62",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98450803",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f0570e1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "157793b9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9be8f45e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
