{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "header",
   "metadata": {},
   "source": [
    "# Logistic Regression Baseline - Customer Purchase Propensity Prediction\n",
    "\n",
    "This notebook trains a Logistic Regression model to predict whether a customer will purchase a product after adding it to cart.\n",
    "\n",
    "## Plan:\n",
    "1. Load data from Feast Feature Store\n",
    "2. Preprocessing: StandardScaler for numerical, OneHotEncoder for categorical\n",
    "3. Train/Val/Test split: 64%/16%/20%\n",
    "4. Regularization tuning on validation set\n",
    "5. Evaluate with Accuracy, Precision, Recall, F1, AUC-ROC\n",
    "6. Save metrics to JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "imports",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries imported successfully!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "\n",
    "from feast import FeatureStore\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    f1_score,\n",
    "    roc_auc_score,\n",
    "    classification_report,\n",
    "    confusion_matrix\n",
    ")\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "data-loading-header",
   "metadata": {},
   "source": [
    "## 1. Load Data from Feast Feature Store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "load-feast",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature store path: ../../data_pipeline/propensity_feature_store/propensity_features/feature_repo/feature_store.yaml\n",
      "Exists: True\n",
      "\n",
      "Feast Feature Store initialized successfully!\n",
      "Project: propensity_features\n"
     ]
    }
   ],
   "source": [
    "# Define paths\n",
    "FEATURE_REPO_PATH = Path(\"../../data_pipeline/propensity_feature_store/propensity_features/feature_repo\")\n",
    "FEATURE_STORE_YAML = FEATURE_REPO_PATH / \"feature_store.yaml\"\n",
    "\n",
    "print(f\"Feature store path: {FEATURE_STORE_YAML}\")\n",
    "print(f\"Exists: {FEATURE_STORE_YAML.exists()}\")\n",
    "\n",
    "# Initialize Feast Feature Store\n",
    "store = FeatureStore(repo_path=str(FEATURE_REPO_PATH))\n",
    "print(\"\\nFeast Feature Store initialized successfully!\")\n",
    "print(f\"Project: {store.project}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "load-parquet",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data from: ../../data_pipeline/propensity_feature_store/propensity_features/feature_repo/data/processed_purchase_propensity_data_v1.parquet\n",
      "\n",
      "Dataset shape: (2933439, 11)\n",
      "\n",
      "Columns: ['user_id', 'product_id', 'event_timestamp', 'created_timestamp', 'category_code_level1', 'category_code_level2', 'brand', 'event_weekday', 'price', 'activity_count', 'is_purchased']\n",
      "\n",
      "Data types:\n",
      "user_id                          int64\n",
      "product_id                       int64\n",
      "event_timestamp         datetime64[ns]\n",
      "created_timestamp       datetime64[us]\n",
      "category_code_level1            object\n",
      "category_code_level2            object\n",
      "brand                           object\n",
      "event_weekday                    int64\n",
      "price                          float64\n",
      "activity_count                   int64\n",
      "is_purchased                     int64\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Load data directly from parquet file (faster than get_historical_features for full dataset)\n",
    "PARQUET_PATH = FEATURE_REPO_PATH / \"data\" / \"processed_purchase_propensity_data_v1.parquet\"\n",
    "\n",
    "print(f\"Loading data from: {PARQUET_PATH}\")\n",
    "df = pd.read_parquet(PARQUET_PATH)\n",
    "\n",
    "print(f\"\\nDataset shape: {df.shape}\")\n",
    "print(f\"\\nColumns: {df.columns.tolist()}\")\n",
    "print(f\"\\nData types:\\n{df.dtypes}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "explore-data",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 5 rows:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>product_id</th>\n",
       "      <th>event_timestamp</th>\n",
       "      <th>created_timestamp</th>\n",
       "      <th>category_code_level1</th>\n",
       "      <th>category_code_level2</th>\n",
       "      <th>brand</th>\n",
       "      <th>event_weekday</th>\n",
       "      <th>price</th>\n",
       "      <th>activity_count</th>\n",
       "      <th>is_purchased</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>515903856</td>\n",
       "      <td>2601552</td>\n",
       "      <td>2019-11-17 00:11:39</td>\n",
       "      <td>2026-01-18 22:17:22.150556</td>\n",
       "      <td>unknown</td>\n",
       "      <td>unknown</td>\n",
       "      <td>gorenje</td>\n",
       "      <td>6</td>\n",
       "      <td>486.24</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>516301799</td>\n",
       "      <td>12702930</td>\n",
       "      <td>2019-11-12 15:40:15</td>\n",
       "      <td>2026-01-18 22:17:22.150556</td>\n",
       "      <td>unknown</td>\n",
       "      <td>unknown</td>\n",
       "      <td>cordiant</td>\n",
       "      <td>1</td>\n",
       "      <td>35.78</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>516301799</td>\n",
       "      <td>12702930</td>\n",
       "      <td>2019-11-12 15:41:46</td>\n",
       "      <td>2026-01-18 22:17:22.150556</td>\n",
       "      <td>unknown</td>\n",
       "      <td>unknown</td>\n",
       "      <td>cordiant</td>\n",
       "      <td>1</td>\n",
       "      <td>35.78</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>516301799</td>\n",
       "      <td>12702930</td>\n",
       "      <td>2019-11-12 15:42:05</td>\n",
       "      <td>2026-01-18 22:17:22.150556</td>\n",
       "      <td>unknown</td>\n",
       "      <td>unknown</td>\n",
       "      <td>cordiant</td>\n",
       "      <td>1</td>\n",
       "      <td>35.78</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>561066382</td>\n",
       "      <td>3800966</td>\n",
       "      <td>2019-11-15 23:36:25</td>\n",
       "      <td>2026-01-18 22:17:22.150556</td>\n",
       "      <td>appliances</td>\n",
       "      <td>iron</td>\n",
       "      <td>elenberg</td>\n",
       "      <td>4</td>\n",
       "      <td>20.57</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     user_id  product_id     event_timestamp          created_timestamp  \\\n",
       "0  515903856     2601552 2019-11-17 00:11:39 2026-01-18 22:17:22.150556   \n",
       "1  516301799    12702930 2019-11-12 15:40:15 2026-01-18 22:17:22.150556   \n",
       "2  516301799    12702930 2019-11-12 15:41:46 2026-01-18 22:17:22.150556   \n",
       "3  516301799    12702930 2019-11-12 15:42:05 2026-01-18 22:17:22.150556   \n",
       "4  561066382     3800966 2019-11-15 23:36:25 2026-01-18 22:17:22.150556   \n",
       "\n",
       "  category_code_level1 category_code_level2     brand  event_weekday   price  \\\n",
       "0              unknown              unknown   gorenje              6  486.24   \n",
       "1              unknown              unknown  cordiant              1   35.78   \n",
       "2              unknown              unknown  cordiant              1   35.78   \n",
       "3              unknown              unknown  cordiant              1   35.78   \n",
       "4           appliances                 iron  elenberg              4   20.57   \n",
       "\n",
       "   activity_count  is_purchased  \n",
       "0               6             0  \n",
       "1               2             0  \n",
       "2               6             0  \n",
       "3               8             0  \n",
       "4               2             0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Target distribution (is_purchased):\n",
      "is_purchased\n",
      "0    2170105\n",
      "1     763334\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Positive rate: 0.2602 (26.02%)\n"
     ]
    }
   ],
   "source": [
    "# Explore the data\n",
    "print(\"First 5 rows:\")\n",
    "display(df.head())\n",
    "\n",
    "print(f\"\\nTarget distribution (is_purchased):\")\n",
    "print(df['is_purchased'].value_counts())\n",
    "print(f\"\\nPositive rate: {df['is_purchased'].mean():.4f} ({df['is_purchased'].mean()*100:.2f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "check-missing",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values:\n",
      "user_id                 0\n",
      "product_id              0\n",
      "event_timestamp         0\n",
      "created_timestamp       0\n",
      "category_code_level1    0\n",
      "category_code_level2    0\n",
      "brand                   0\n",
      "event_weekday           0\n",
      "price                   0\n",
      "activity_count          0\n",
      "is_purchased            0\n",
      "dtype: int64\n",
      "\n",
      "Unique values per categorical column:\n",
      "  brand: 3058 unique values\n",
      "  category_code_level1: 14 unique values\n",
      "  category_code_level2: 58 unique values\n"
     ]
    }
   ],
   "source": [
    "# Check for missing values\n",
    "print(\"Missing values:\")\n",
    "print(df.isnull().sum())\n",
    "\n",
    "print(f\"\\nUnique values per categorical column:\")\n",
    "for col in ['brand', 'category_code_level1', 'category_code_level2']:\n",
    "    print(f\"  {col}: {df[col].nunique()} unique values\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "preprocessing-header",
   "metadata": {},
   "source": [
    "## 2. Preprocessing Pipeline\n",
    "\n",
    "- **Numerical features**: StandardScaler for `price`, `activity_count`, `event_weekday`\n",
    "- **Categorical features**: OneHotEncoder for `brand`, `category_code_level1`, `category_code_level2`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "define-features",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numerical features: ['price', 'activity_count', 'event_weekday']\n",
      "Categorical features: ['brand', 'category_code_level1', 'category_code_level2']\n",
      "Target: is_purchased\n"
     ]
    }
   ],
   "source": [
    "# Define feature columns\n",
    "NUMERICAL_FEATURES = ['price', 'activity_count', 'event_weekday']\n",
    "CATEGORICAL_FEATURES = ['brand', 'category_code_level1', 'category_code_level2']\n",
    "TARGET = 'is_purchased'\n",
    "\n",
    "ALL_FEATURES = NUMERICAL_FEATURES + CATEGORICAL_FEATURES\n",
    "\n",
    "print(f\"Numerical features: {NUMERICAL_FEATURES}\")\n",
    "print(f\"Categorical features: {CATEGORICAL_FEATURES}\")\n",
    "print(f\"Target: {TARGET}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "prepare-data",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shape: (2933439, 6)\n",
      "y shape: (2933439,)\n",
      "\n",
      "X dtypes:\n",
      "price                   float64\n",
      "activity_count            int64\n",
      "event_weekday             int64\n",
      "brand                    object\n",
      "category_code_level1     object\n",
      "category_code_level2     object\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Prepare X and y\n",
    "X = df[ALL_FEATURES].copy()\n",
    "y = df[TARGET].copy()\n",
    "\n",
    "# Convert categorical columns to string type for OneHotEncoder\n",
    "for col in CATEGORICAL_FEATURES:\n",
    "    X[col] = X[col].astype(str)\n",
    "\n",
    "print(f\"X shape: {X.shape}\")\n",
    "print(f\"y shape: {y.shape}\")\n",
    "print(f\"\\nX dtypes:\\n{X.dtypes}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "split-header",
   "metadata": {},
   "source": [
    "## 3. Train/Validation/Test Split (64%/16%/20%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "split-data",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set:   1,877,400 samples (64.0%)\n",
      "Validation set: 469,351 samples (16.0%)\n",
      "Test set:       586,688 samples (20.0%)\n",
      "\n",
      "Class distribution:\n",
      "  Train - Positive rate: 0.2602\n",
      "  Val   - Positive rate: 0.2602\n",
      "  Test  - Positive rate: 0.2602\n"
     ]
    }
   ],
   "source": [
    "# First split: 80% train+val, 20% test\n",
    "X_train_val, X_test, y_train_val, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# Second split: 80% train, 20% val (of the 80% = 64% and 16% of total)\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_train_val, y_train_val, test_size=0.2, random_state=42, stratify=y_train_val\n",
    ")\n",
    "\n",
    "print(f\"Training set:   {X_train.shape[0]:,} samples ({X_train.shape[0]/len(X)*100:.1f}%)\")\n",
    "print(f\"Validation set: {X_val.shape[0]:,} samples ({X_val.shape[0]/len(X)*100:.1f}%)\")\n",
    "print(f\"Test set:       {X_test.shape[0]:,} samples ({X_test.shape[0]/len(X)*100:.1f}%)\")\n",
    "\n",
    "print(f\"\\nClass distribution:\")\n",
    "print(f\"  Train - Positive rate: {y_train.mean():.4f}\")\n",
    "print(f\"  Val   - Positive rate: {y_val.mean():.4f}\")\n",
    "print(f\"  Test  - Positive rate: {y_test.mean():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "create-preprocessor",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessor created!\n",
      "ColumnTransformer(transformers=[('num', StandardScaler(),\n",
      "                                 ['price', 'activity_count', 'event_weekday']),\n",
      "                                ('cat',\n",
      "                                 OneHotEncoder(handle_unknown='ignore',\n",
      "                                               sparse_output=False),\n",
      "                                 ['brand', 'category_code_level1',\n",
      "                                  'category_code_level2'])])\n"
     ]
    }
   ],
   "source": [
    "# Create preprocessing pipeline\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', StandardScaler(), NUMERICAL_FEATURES),\n",
    "        ('cat', OneHotEncoder(handle_unknown='ignore', sparse_output=False), CATEGORICAL_FEATURES)\n",
    "    ],\n",
    "    remainder='drop'\n",
    ")\n",
    "\n",
    "print(\"Preprocessor created!\")\n",
    "print(preprocessor)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "regularization-header",
   "metadata": {},
   "source": [
    "## 4. Regularization Tuning on Validation Set\n",
    "\n",
    "Try different values of C (inverse of regularization strength) and select the best one based on validation AUC-ROC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "regularization-tuning",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting regularization tuning...\n",
      "============================================================\n",
      "\n",
      "Training with C=0.001...\n",
      "  Accuracy: 0.5580\n",
      "  F1 Macro: 0.5246\n",
      "  AUC-ROC:  0.5798\n",
      "\n",
      "Training with C=0.01...\n",
      "  Accuracy: 0.5534\n",
      "  F1 Macro: 0.5228\n",
      "  AUC-ROC:  0.5832\n",
      "\n",
      "Training with C=0.1...\n",
      "  Accuracy: 0.5503\n",
      "  F1 Macro: 0.5211\n",
      "  AUC-ROC:  0.5843\n",
      "\n",
      "Training with C=1...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[21]\u001b[39m\u001b[32m, line 27\u001b[39m\n\u001b[32m     14\u001b[39m pipeline = Pipeline([\n\u001b[32m     15\u001b[39m     (\u001b[33m'\u001b[39m\u001b[33mpreprocessor\u001b[39m\u001b[33m'\u001b[39m, preprocessor),\n\u001b[32m     16\u001b[39m     (\u001b[33m'\u001b[39m\u001b[33mclassifier\u001b[39m\u001b[33m'\u001b[39m, LogisticRegression(\n\u001b[32m   (...)\u001b[39m\u001b[32m     23\u001b[39m     ))\n\u001b[32m     24\u001b[39m ])\n\u001b[32m     26\u001b[39m \u001b[38;5;66;03m# Fit on training data\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m27\u001b[39m \u001b[43mpipeline\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     29\u001b[39m \u001b[38;5;66;03m# Predict on validation set\u001b[39;00m\n\u001b[32m     30\u001b[39m y_val_pred = pipeline.predict(X_val)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Library/CloudStorage/GoogleDrive-lethanhquang094@gmail.com/My Drive/FPT/Semester_4/DAP391m/Customer_Purchas_Propensity_Prediction/.venv/lib/python3.11/site-packages/sklearn/base.py:1336\u001b[39m, in \u001b[36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(estimator, *args, **kwargs)\u001b[39m\n\u001b[32m   1329\u001b[39m     estimator._validate_params()\n\u001b[32m   1331\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[32m   1332\u001b[39m     skip_parameter_validation=(\n\u001b[32m   1333\u001b[39m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[32m   1334\u001b[39m     )\n\u001b[32m   1335\u001b[39m ):\n\u001b[32m-> \u001b[39m\u001b[32m1336\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Library/CloudStorage/GoogleDrive-lethanhquang094@gmail.com/My Drive/FPT/Semester_4/DAP391m/Customer_Purchas_Propensity_Prediction/.venv/lib/python3.11/site-packages/sklearn/pipeline.py:621\u001b[39m, in \u001b[36mPipeline.fit\u001b[39m\u001b[34m(self, X, y, **params)\u001b[39m\n\u001b[32m    615\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._final_estimator != \u001b[33m\"\u001b[39m\u001b[33mpassthrough\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    616\u001b[39m         last_step_params = \u001b[38;5;28mself\u001b[39m._get_metadata_for_step(\n\u001b[32m    617\u001b[39m             step_idx=\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m) - \u001b[32m1\u001b[39m,\n\u001b[32m    618\u001b[39m             step_params=routed_params[\u001b[38;5;28mself\u001b[39m.steps[-\u001b[32m1\u001b[39m][\u001b[32m0\u001b[39m]],\n\u001b[32m    619\u001b[39m             all_params=params,\n\u001b[32m    620\u001b[39m         )\n\u001b[32m--> \u001b[39m\u001b[32m621\u001b[39m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_final_estimator\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mXt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mlast_step_params\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfit\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    623\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Library/CloudStorage/GoogleDrive-lethanhquang094@gmail.com/My Drive/FPT/Semester_4/DAP391m/Customer_Purchas_Propensity_Prediction/.venv/lib/python3.11/site-packages/sklearn/base.py:1336\u001b[39m, in \u001b[36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(estimator, *args, **kwargs)\u001b[39m\n\u001b[32m   1329\u001b[39m     estimator._validate_params()\n\u001b[32m   1331\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[32m   1332\u001b[39m     skip_parameter_validation=(\n\u001b[32m   1333\u001b[39m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[32m   1334\u001b[39m     )\n\u001b[32m   1335\u001b[39m ):\n\u001b[32m-> \u001b[39m\u001b[32m1336\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Library/CloudStorage/GoogleDrive-lethanhquang094@gmail.com/My Drive/FPT/Semester_4/DAP391m/Customer_Purchas_Propensity_Prediction/.venv/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:1262\u001b[39m, in \u001b[36mLogisticRegression.fit\u001b[39m\u001b[34m(self, X, y, sample_weight)\u001b[39m\n\u001b[32m   1258\u001b[39m \u001b[38;5;66;03m# TODO: enable multi-threading if benchmarks show a positive effect,\u001b[39;00m\n\u001b[32m   1259\u001b[39m \u001b[38;5;66;03m# see https://github.com/scikit-learn/scikit-learn/issues/32162\u001b[39;00m\n\u001b[32m   1260\u001b[39m n_threads = \u001b[32m1\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m1262\u001b[39m coefs, _, n_iter = \u001b[43m_logistic_regression_path\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1263\u001b[39m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1264\u001b[39m \u001b[43m    \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1265\u001b[39m \u001b[43m    \u001b[49m\u001b[43mclasses\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mclasses_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1266\u001b[39m \u001b[43m    \u001b[49m\u001b[43mCs\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[43mC_\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1267\u001b[39m \u001b[43m    \u001b[49m\u001b[43ml1_ratio\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43ml1_ratio\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1268\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfit_intercept\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfit_intercept\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1269\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtol\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtol\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1270\u001b[39m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1271\u001b[39m \u001b[43m    \u001b[49m\u001b[43msolver\u001b[49m\u001b[43m=\u001b[49m\u001b[43msolver\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1272\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmax_iter\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmax_iter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1273\u001b[39m \u001b[43m    \u001b[49m\u001b[43mclass_weight\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mclass_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1274\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcheck_input\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   1275\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1276\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcoef\u001b[49m\u001b[43m=\u001b[49m\u001b[43mwarm_start_coef\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1277\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpenalty\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpenalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1278\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmax_squared_sum\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmax_squared_sum\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1279\u001b[39m \u001b[43m    \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m=\u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1280\u001b[39m \u001b[43m    \u001b[49m\u001b[43mn_threads\u001b[49m\u001b[43m=\u001b[49m\u001b[43mn_threads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1281\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1283\u001b[39m \u001b[38;5;28mself\u001b[39m.n_iter_ = np.asarray(n_iter, dtype=np.int32)\n\u001b[32m   1285\u001b[39m \u001b[38;5;28mself\u001b[39m.coef_ = coefs[\u001b[32m0\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Library/CloudStorage/GoogleDrive-lethanhquang094@gmail.com/My Drive/FPT/Semester_4/DAP391m/Customer_Purchas_Propensity_Prediction/.venv/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:392\u001b[39m, in \u001b[36m_logistic_regression_path\u001b[39m\u001b[34m(X, y, classes, Cs, fit_intercept, max_iter, tol, verbose, solver, coef, class_weight, dual, penalty, intercept_scaling, random_state, check_input, max_squared_sum, sample_weight, l1_ratio, n_threads)\u001b[39m\n\u001b[32m    388\u001b[39m l2_reg_strength = \u001b[32m1.0\u001b[39m / (C * sw_sum)\n\u001b[32m    389\u001b[39m iprint = [-\u001b[32m1\u001b[39m, \u001b[32m50\u001b[39m, \u001b[32m1\u001b[39m, \u001b[32m100\u001b[39m, \u001b[32m101\u001b[39m][\n\u001b[32m    390\u001b[39m     np.searchsorted(np.array([\u001b[32m0\u001b[39m, \u001b[32m1\u001b[39m, \u001b[32m2\u001b[39m, \u001b[32m3\u001b[39m]), verbose)\n\u001b[32m    391\u001b[39m ]\n\u001b[32m--> \u001b[39m\u001b[32m392\u001b[39m opt_res = \u001b[43moptimize\u001b[49m\u001b[43m.\u001b[49m\u001b[43mminimize\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    393\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    394\u001b[39m \u001b[43m    \u001b[49m\u001b[43mw0\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    395\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mL-BFGS-B\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    396\u001b[39m \u001b[43m    \u001b[49m\u001b[43mjac\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    397\u001b[39m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[43m=\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43ml2_reg_strength\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_threads\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    398\u001b[39m \u001b[43m    \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m=\u001b[49m\u001b[43m{\u001b[49m\n\u001b[32m    399\u001b[39m \u001b[43m        \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmaxiter\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_iter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    400\u001b[39m \u001b[43m        \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmaxls\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m50\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# default is 20\u001b[39;49;00m\n\u001b[32m    401\u001b[39m \u001b[43m        \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mgtol\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtol\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    402\u001b[39m \u001b[43m        \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mftol\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m64\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m \u001b[49m\u001b[43mnp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfinfo\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mfloat\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    403\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43m_get_additional_lbfgs_options_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43miprint\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43miprint\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    404\u001b[39m \u001b[43m    \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    405\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    406\u001b[39m n_iter_i = _check_optimize_result(\n\u001b[32m    407\u001b[39m     solver,\n\u001b[32m    408\u001b[39m     opt_res,\n\u001b[32m    409\u001b[39m     max_iter,\n\u001b[32m    410\u001b[39m     extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n\u001b[32m    411\u001b[39m )\n\u001b[32m    412\u001b[39m w0, loss = opt_res.x, opt_res.fun\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Library/CloudStorage/GoogleDrive-lethanhquang094@gmail.com/My Drive/FPT/Semester_4/DAP391m/Customer_Purchas_Propensity_Prediction/.venv/lib/python3.11/site-packages/scipy/optimize/_minimize.py:784\u001b[39m, in \u001b[36mminimize\u001b[39m\u001b[34m(fun, x0, args, method, jac, hess, hessp, bounds, constraints, tol, callback, options)\u001b[39m\n\u001b[32m    781\u001b[39m     res = _minimize_newtoncg(fun, x0, args, jac, hess, hessp, callback,\n\u001b[32m    782\u001b[39m                              **options)\n\u001b[32m    783\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m meth == \u001b[33m'\u001b[39m\u001b[33ml-bfgs-b\u001b[39m\u001b[33m'\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m784\u001b[39m     res = \u001b[43m_minimize_lbfgsb\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfun\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjac\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbounds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    785\u001b[39m \u001b[43m                           \u001b[49m\u001b[43mcallback\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    786\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m meth == \u001b[33m'\u001b[39m\u001b[33mtnc\u001b[39m\u001b[33m'\u001b[39m:\n\u001b[32m    787\u001b[39m     res = _minimize_tnc(fun, x0, args, jac, bounds, callback=callback,\n\u001b[32m    788\u001b[39m                         **options)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Library/CloudStorage/GoogleDrive-lethanhquang094@gmail.com/My Drive/FPT/Semester_4/DAP391m/Customer_Purchas_Propensity_Prediction/.venv/lib/python3.11/site-packages/scipy/optimize/_lbfgsb_py.py:469\u001b[39m, in \u001b[36m_minimize_lbfgsb\u001b[39m\u001b[34m(fun, x0, args, jac, bounds, disp, maxcor, ftol, gtol, eps, maxfun, maxiter, iprint, callback, maxls, finite_diff_rel_step, workers, **unknown_options)\u001b[39m\n\u001b[32m    461\u001b[39m _lbfgsb.setulb(m, x, low_bnd, upper_bnd, nbd, f, g, factr, pgtol, wa,\n\u001b[32m    462\u001b[39m                iwa, task, lsave, isave, dsave, maxls, ln_task)\n\u001b[32m    464\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m task[\u001b[32m0\u001b[39m] == \u001b[32m3\u001b[39m:\n\u001b[32m    465\u001b[39m     \u001b[38;5;66;03m# The minimization routine wants f and g at the current x.\u001b[39;00m\n\u001b[32m    466\u001b[39m     \u001b[38;5;66;03m# Note that interruptions due to maxfun are postponed\u001b[39;00m\n\u001b[32m    467\u001b[39m     \u001b[38;5;66;03m# until the completion of the current minimization iteration.\u001b[39;00m\n\u001b[32m    468\u001b[39m     \u001b[38;5;66;03m# Overwrite f and g:\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m469\u001b[39m     f, g = \u001b[43mfunc_and_grad\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    470\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m task[\u001b[32m0\u001b[39m] == \u001b[32m1\u001b[39m:\n\u001b[32m    471\u001b[39m     \u001b[38;5;66;03m# new iteration\u001b[39;00m\n\u001b[32m    472\u001b[39m     n_iterations += \u001b[32m1\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Library/CloudStorage/GoogleDrive-lethanhquang094@gmail.com/My Drive/FPT/Semester_4/DAP391m/Customer_Purchas_Propensity_Prediction/.venv/lib/python3.11/site-packages/scipy/optimize/_differentiable_functions.py:412\u001b[39m, in \u001b[36mScalarFunction.fun_and_grad\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m    410\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m np.array_equal(x, \u001b[38;5;28mself\u001b[39m.x):\n\u001b[32m    411\u001b[39m     \u001b[38;5;28mself\u001b[39m._update_x(x)\n\u001b[32m--> \u001b[39m\u001b[32m412\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_update_fun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    413\u001b[39m \u001b[38;5;28mself\u001b[39m._update_grad()\n\u001b[32m    414\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.f, \u001b[38;5;28mself\u001b[39m.g\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Library/CloudStorage/GoogleDrive-lethanhquang094@gmail.com/My Drive/FPT/Semester_4/DAP391m/Customer_Purchas_Propensity_Prediction/.venv/lib/python3.11/site-packages/scipy/optimize/_differentiable_functions.py:362\u001b[39m, in \u001b[36mScalarFunction._update_fun\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    360\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_update_fun\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    361\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m.f_updated:\n\u001b[32m--> \u001b[39m\u001b[32m362\u001b[39m         fx = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_wrapped_fun\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    363\u001b[39m         \u001b[38;5;28mself\u001b[39m._nfev += \u001b[32m1\u001b[39m\n\u001b[32m    364\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m fx < \u001b[38;5;28mself\u001b[39m._lowest_f:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Library/CloudStorage/GoogleDrive-lethanhquang094@gmail.com/My Drive/FPT/Semester_4/DAP391m/Customer_Purchas_Propensity_Prediction/.venv/lib/python3.11/site-packages/scipy/_lib/_util.py:603\u001b[39m, in \u001b[36m_ScalarFunctionWrapper.__call__\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m    600\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[32m    601\u001b[39m     \u001b[38;5;66;03m# Send a copy because the user may overwrite it.\u001b[39;00m\n\u001b[32m    602\u001b[39m     \u001b[38;5;66;03m# The user of this class might want `x` to remain unchanged.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m603\u001b[39m     fx = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    604\u001b[39m     \u001b[38;5;28mself\u001b[39m.nfev += \u001b[32m1\u001b[39m\n\u001b[32m    606\u001b[39m     \u001b[38;5;66;03m# Make sure the function returns a true scalar\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Library/CloudStorage/GoogleDrive-lethanhquang094@gmail.com/My Drive/FPT/Semester_4/DAP391m/Customer_Purchas_Propensity_Prediction/.venv/lib/python3.11/site-packages/scipy/optimize/_optimize.py:80\u001b[39m, in \u001b[36mMemoizeJac.__call__\u001b[39m\u001b[34m(self, x, *args)\u001b[39m\n\u001b[32m     78\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, *args):\n\u001b[32m     79\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\" returns the function value \"\"\"\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m80\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_compute_if_needed\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     81\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._value\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Library/CloudStorage/GoogleDrive-lethanhquang094@gmail.com/My Drive/FPT/Semester_4/DAP391m/Customer_Purchas_Propensity_Prediction/.venv/lib/python3.11/site-packages/scipy/optimize/_optimize.py:74\u001b[39m, in \u001b[36mMemoizeJac._compute_if_needed\u001b[39m\u001b[34m(self, x, *args)\u001b[39m\n\u001b[32m     72\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m np.all(x == \u001b[38;5;28mself\u001b[39m.x) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m.jac \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m     73\u001b[39m     \u001b[38;5;28mself\u001b[39m.x = np.asarray(x).copy()\n\u001b[32m---> \u001b[39m\u001b[32m74\u001b[39m     fg = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     75\u001b[39m     \u001b[38;5;28mself\u001b[39m.jac = fg[\u001b[32m1\u001b[39m]\n\u001b[32m     76\u001b[39m     \u001b[38;5;28mself\u001b[39m._value = fg[\u001b[32m0\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Library/CloudStorage/GoogleDrive-lethanhquang094@gmail.com/My Drive/FPT/Semester_4/DAP391m/Customer_Purchas_Propensity_Prediction/.venv/lib/python3.11/site-packages/sklearn/linear_model/_linear_loss.py:319\u001b[39m, in \u001b[36mLinearModelLoss.loss_gradient\u001b[39m\u001b[34m(self, coef, X, y, sample_weight, l2_reg_strength, n_threads, raw_prediction)\u001b[39m\n\u001b[32m    316\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    317\u001b[39m     weights, intercept = \u001b[38;5;28mself\u001b[39m.weight_intercept(coef)\n\u001b[32m--> \u001b[39m\u001b[32m319\u001b[39m loss, grad_pointwise = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbase_loss\u001b[49m\u001b[43m.\u001b[49m\u001b[43mloss_gradient\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    320\u001b[39m \u001b[43m    \u001b[49m\u001b[43my_true\u001b[49m\u001b[43m=\u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    321\u001b[39m \u001b[43m    \u001b[49m\u001b[43mraw_prediction\u001b[49m\u001b[43m=\u001b[49m\u001b[43mraw_prediction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    322\u001b[39m \u001b[43m    \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m=\u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    323\u001b[39m \u001b[43m    \u001b[49m\u001b[43mn_threads\u001b[49m\u001b[43m=\u001b[49m\u001b[43mn_threads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    324\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    325\u001b[39m sw_sum = n_samples \u001b[38;5;28;01mif\u001b[39;00m sample_weight \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m np.sum(sample_weight)\n\u001b[32m    326\u001b[39m loss = loss.sum() / sw_sum\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Library/CloudStorage/GoogleDrive-lethanhquang094@gmail.com/My Drive/FPT/Semester_4/DAP391m/Customer_Purchas_Propensity_Prediction/.venv/lib/python3.11/site-packages/sklearn/_loss/loss.py:205\u001b[39m, in \u001b[36mBaseLoss.loss_gradient\u001b[39m\u001b[34m(self, y_true, raw_prediction, sample_weight, loss_out, gradient_out, n_threads)\u001b[39m\n\u001b[32m    196\u001b[39m     \u001b[38;5;28mself\u001b[39m.closs.loss(\n\u001b[32m    197\u001b[39m         y_true=y_true,\n\u001b[32m    198\u001b[39m         raw_prediction=raw_prediction,\n\u001b[32m   (...)\u001b[39m\u001b[32m    201\u001b[39m         n_threads=n_threads,\n\u001b[32m    202\u001b[39m     )\n\u001b[32m    203\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m loss_out\n\u001b[32m--> \u001b[39m\u001b[32m205\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mloss_gradient\u001b[39m(\n\u001b[32m    206\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    207\u001b[39m     y_true,\n\u001b[32m    208\u001b[39m     raw_prediction,\n\u001b[32m    209\u001b[39m     sample_weight=\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    210\u001b[39m     loss_out=\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    211\u001b[39m     gradient_out=\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    212\u001b[39m     n_threads=\u001b[32m1\u001b[39m,\n\u001b[32m    213\u001b[39m ):\n\u001b[32m    214\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Compute loss and gradient w.r.t. raw_prediction for each input.\u001b[39;00m\n\u001b[32m    215\u001b[39m \n\u001b[32m    216\u001b[39m \u001b[33;03m    Parameters\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    241\u001b[39m \u001b[33;03m        Element-wise gradients.\u001b[39;00m\n\u001b[32m    242\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m    243\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m loss_out \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# Define C values to try\n",
    "C_VALUES = [0.001, 0.01, 0.1, 1, 10, 100]\n",
    "\n",
    "# Store results\n",
    "tuning_results = []\n",
    "\n",
    "print(\"Starting regularization tuning...\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for C in C_VALUES:\n",
    "    print(f\"\\nTraining with C={C}...\")\n",
    "    \n",
    "    # Create pipeline with current C\n",
    "    pipeline = Pipeline([\n",
    "        ('preprocessor', preprocessor),\n",
    "        ('classifier', LogisticRegression(\n",
    "            C=C,\n",
    "            solver='lbfgs',\n",
    "            max_iter=1000,\n",
    "            class_weight='balanced',\n",
    "            random_state=42,\n",
    "            n_jobs=-1\n",
    "        ))\n",
    "    ])\n",
    "    \n",
    "    # Fit on training data\n",
    "    pipeline.fit(X_train, y_train)\n",
    "    \n",
    "    # Predict on validation set\n",
    "    y_val_pred = pipeline.predict(X_val)\n",
    "    y_val_proba = pipeline.predict_proba(X_val)[:, 1]\n",
    "    \n",
    "    # Calculate metrics\n",
    "    val_accuracy = accuracy_score(y_val, y_val_pred)\n",
    "    val_f1 = f1_score(y_val, y_val_pred, average='macro')\n",
    "    val_auc = roc_auc_score(y_val, y_val_proba)\n",
    "    \n",
    "    result = {\n",
    "        'C': C,\n",
    "        'accuracy': val_accuracy,\n",
    "        'f1_macro': val_f1,\n",
    "        'auc_roc': val_auc,\n",
    "        'pipeline': pipeline\n",
    "    }\n",
    "    tuning_results.append(result)\n",
    "    \n",
    "    print(f\"  Accuracy: {val_accuracy:.4f}\")\n",
    "    print(f\"  F1 Macro: {val_f1:.4f}\")\n",
    "    print(f\"  AUC-ROC:  {val_auc:.4f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Tuning complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "select-best",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select best model based on AUC-ROC\n",
    "best_result = max(tuning_results, key=lambda x: x['auc_roc'])\n",
    "best_C = best_result['C']\n",
    "best_pipeline = best_result['pipeline']\n",
    "\n",
    "print(f\"Best C value: {best_C}\")\n",
    "print(f\"Best validation AUC-ROC: {best_result['auc_roc']:.4f}\")\n",
    "\n",
    "# Summary table\n",
    "print(\"\\nTuning Summary:\")\n",
    "print(\"-\" * 50)\n",
    "print(f\"{'C':>10} | {'Accuracy':>10} | {'F1 Macro':>10} | {'AUC-ROC':>10}\")\n",
    "print(\"-\" * 50)\n",
    "for r in tuning_results:\n",
    "    marker = \" <-- BEST\" if r['C'] == best_C else \"\"\n",
    "    print(f\"{r['C']:>10} | {r['accuracy']:>10.4f} | {r['f1_macro']:>10.4f} | {r['auc_roc']:>10.4f}{marker}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "final-training-header",
   "metadata": {},
   "source": [
    "## 5. Final Training and Evaluation\n",
    "\n",
    "Train on train+validation, evaluate on test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "final-training",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine train and validation for final training\n",
    "X_train_final = pd.concat([X_train, X_val], axis=0)\n",
    "y_train_final = pd.concat([y_train, y_val], axis=0)\n",
    "\n",
    "print(f\"Final training set size: {len(X_train_final):,} samples\")\n",
    "\n",
    "# Create final pipeline with best C\n",
    "final_pipeline = Pipeline([\n",
    "    ('preprocessor', ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('num', StandardScaler(), NUMERICAL_FEATURES),\n",
    "            ('cat', OneHotEncoder(handle_unknown='ignore', sparse_output=False), CATEGORICAL_FEATURES)\n",
    "        ],\n",
    "        remainder='drop'\n",
    "    )),\n",
    "    ('classifier', LogisticRegression(\n",
    "        C=best_C,\n",
    "        solver='lbfgs',\n",
    "        max_iter=1000,\n",
    "        class_weight='balanced',\n",
    "        random_state=42,\n",
    "        n_jobs=-1\n",
    "    ))\n",
    "])\n",
    "\n",
    "# Train final model\n",
    "print(\"\\nTraining final model...\")\n",
    "final_pipeline.fit(X_train_final, y_train_final)\n",
    "print(\"Training complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "evaluate-test",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on test set\n",
    "print(\"Evaluating on test set...\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "y_test_pred = final_pipeline.predict(X_test)\n",
    "y_test_proba = final_pipeline.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Calculate all metrics\n",
    "test_accuracy = accuracy_score(y_test, y_test_pred)\n",
    "test_precision_macro = precision_score(y_test, y_test_pred, average='macro')\n",
    "test_recall_macro = recall_score(y_test, y_test_pred, average='macro')\n",
    "test_f1_macro = f1_score(y_test, y_test_pred, average='macro')\n",
    "test_auc_roc = roc_auc_score(y_test, y_test_proba)\n",
    "\n",
    "# Per-class metrics\n",
    "test_precision_per_class = precision_score(y_test, y_test_pred, average=None)\n",
    "test_recall_per_class = recall_score(y_test, y_test_pred, average=None)\n",
    "test_f1_per_class = f1_score(y_test, y_test_pred, average=None)\n",
    "\n",
    "print(f\"\\nTest Set Metrics:\")\n",
    "print(f\"  Accuracy:  {test_accuracy:.4f}\")\n",
    "print(f\"  Precision: {test_precision_macro:.4f} (macro)\")\n",
    "print(f\"  Recall:    {test_recall_macro:.4f} (macro)\")\n",
    "print(f\"  F1-Score:  {test_f1_macro:.4f} (macro)\")\n",
    "print(f\"  AUC-ROC:   {test_auc_roc:.4f}\")\n",
    "\n",
    "print(f\"\\nPer-Class Metrics:\")\n",
    "print(f\"  Class 0 (Not Purchased): Precision={test_precision_per_class[0]:.4f}, Recall={test_recall_per_class[0]:.4f}, F1={test_f1_per_class[0]:.4f}\")\n",
    "print(f\"  Class 1 (Purchased):     Precision={test_precision_per_class[1]:.4f}, Recall={test_recall_per_class[1]:.4f}, F1={test_f1_per_class[1]:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "confusion-matrix",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion Matrix\n",
    "print(\"Confusion Matrix:\")\n",
    "cm = confusion_matrix(y_test, y_test_pred)\n",
    "print(cm)\n",
    "\n",
    "print(f\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_test_pred, target_names=['Not Purchased', 'Purchased']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "validation-metrics",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Also get validation metrics from the best model for comparison\n",
    "y_val_pred_best = best_pipeline.predict(X_val)\n",
    "y_val_proba_best = best_pipeline.predict_proba(X_val)[:, 1]\n",
    "\n",
    "val_accuracy = accuracy_score(y_val, y_val_pred_best)\n",
    "val_precision_macro = precision_score(y_val, y_val_pred_best, average='macro')\n",
    "val_recall_macro = recall_score(y_val, y_val_pred_best, average='macro')\n",
    "val_f1_macro = f1_score(y_val, y_val_pred_best, average='macro')\n",
    "val_auc_roc = roc_auc_score(y_val, y_val_proba_best)\n",
    "\n",
    "val_precision_per_class = precision_score(y_val, y_val_pred_best, average=None)\n",
    "val_recall_per_class = recall_score(y_val, y_val_pred_best, average=None)\n",
    "val_f1_per_class = f1_score(y_val, y_val_pred_best, average=None)\n",
    "\n",
    "print(\"Validation Set Metrics (for comparison):\")\n",
    "print(f\"  Accuracy:  {val_accuracy:.4f}\")\n",
    "print(f\"  Precision: {val_precision_macro:.4f} (macro)\")\n",
    "print(f\"  Recall:    {val_recall_macro:.4f} (macro)\")\n",
    "print(f\"  F1-Score:  {val_f1_macro:.4f} (macro)\")\n",
    "print(f\"  AUC-ROC:   {val_auc_roc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "save-metrics-header",
   "metadata": {},
   "source": [
    "## 6. Save Metrics to JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "save-metrics",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare metrics dictionary\n",
    "metrics = {\n",
    "    \"model\": \"LogisticRegression\",\n",
    "    \"timestamp\": datetime.now().isoformat(),\n",
    "    \"hyperparameters\": {\n",
    "        \"best_C\": best_C,\n",
    "        \"solver\": \"lbfgs\",\n",
    "        \"max_iter\": 1000,\n",
    "        \"class_weight\": \"balanced\"\n",
    "    },\n",
    "    \"data_split\": {\n",
    "        \"train_size\": int(len(X_train)),\n",
    "        \"val_size\": int(len(X_val)),\n",
    "        \"test_size\": int(len(X_test)),\n",
    "        \"train_val_size\": int(len(X_train_final)),\n",
    "        \"total_size\": int(len(X))\n",
    "    },\n",
    "    \"features\": {\n",
    "        \"numerical\": NUMERICAL_FEATURES,\n",
    "        \"categorical\": CATEGORICAL_FEATURES,\n",
    "        \"preprocessing\": {\n",
    "            \"numerical\": \"StandardScaler\",\n",
    "            \"categorical\": \"OneHotEncoder\"\n",
    "        }\n",
    "    },\n",
    "    \"regularization_tuning\": [\n",
    "        {\n",
    "            \"C\": r['C'],\n",
    "            \"val_accuracy\": round(r['accuracy'], 4),\n",
    "            \"val_f1_macro\": round(r['f1_macro'], 4),\n",
    "            \"val_auc_roc\": round(r['auc_roc'], 4)\n",
    "        }\n",
    "        for r in tuning_results\n",
    "    ],\n",
    "    \"validation_metrics\": {\n",
    "        \"accuracy\": round(val_accuracy, 4),\n",
    "        \"precision\": {\n",
    "            \"macro\": round(val_precision_macro, 4),\n",
    "            \"class_0\": round(float(val_precision_per_class[0]), 4),\n",
    "            \"class_1\": round(float(val_precision_per_class[1]), 4)\n",
    "        },\n",
    "        \"recall\": {\n",
    "            \"macro\": round(val_recall_macro, 4),\n",
    "            \"class_0\": round(float(val_recall_per_class[0]), 4),\n",
    "            \"class_1\": round(float(val_recall_per_class[1]), 4)\n",
    "        },\n",
    "        \"f1\": {\n",
    "            \"macro\": round(val_f1_macro, 4),\n",
    "            \"class_0\": round(float(val_f1_per_class[0]), 4),\n",
    "            \"class_1\": round(float(val_f1_per_class[1]), 4)\n",
    "        },\n",
    "        \"auc_roc\": round(val_auc_roc, 4)\n",
    "    },\n",
    "    \"test_metrics\": {\n",
    "        \"accuracy\": round(test_accuracy, 4),\n",
    "        \"precision\": {\n",
    "            \"macro\": round(test_precision_macro, 4),\n",
    "            \"class_0\": round(float(test_precision_per_class[0]), 4),\n",
    "            \"class_1\": round(float(test_precision_per_class[1]), 4)\n",
    "        },\n",
    "        \"recall\": {\n",
    "            \"macro\": round(test_recall_macro, 4),\n",
    "            \"class_0\": round(float(test_recall_per_class[0]), 4),\n",
    "            \"class_1\": round(float(test_recall_per_class[1]), 4)\n",
    "        },\n",
    "        \"f1\": {\n",
    "            \"macro\": round(test_f1_macro, 4),\n",
    "            \"class_0\": round(float(test_f1_per_class[0]), 4),\n",
    "            \"class_1\": round(float(test_f1_per_class[1]), 4)\n",
    "        },\n",
    "        \"auc_roc\": round(test_auc_roc, 4)\n",
    "    },\n",
    "    \"confusion_matrix\": {\n",
    "        \"true_negative\": int(cm[0, 0]),\n",
    "        \"false_positive\": int(cm[0, 1]),\n",
    "        \"false_negative\": int(cm[1, 0]),\n",
    "        \"true_positive\": int(cm[1, 1])\n",
    "    }\n",
    "}\n",
    "\n",
    "print(\"Metrics prepared!\")\n",
    "print(json.dumps(metrics, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "write-json",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save to JSON file\n",
    "METRICS_PATH = Path(\"../metrics/logistic_regression_metrics.json\")\n",
    "METRICS_PATH.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "with open(METRICS_PATH, 'w') as f:\n",
    "    json.dump(metrics, f, indent=2)\n",
    "\n",
    "print(f\"Metrics saved to: {METRICS_PATH.resolve()}\")\n",
    "print(\"\\nTraining complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "summary-header",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook trained a Logistic Regression baseline model for customer purchase propensity prediction.\n",
    "\n",
    "### Key Results:\n",
    "- Best regularization parameter C was selected based on validation AUC-ROC\n",
    "- Model uses class_weight='balanced' to handle imbalanced data (26% positive rate)\n",
    "- Final model trained on train+validation, evaluated on held-out test set\n",
    "- All metrics saved to `model_pipeline/metrics/logistic_regression_metrics.json`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
